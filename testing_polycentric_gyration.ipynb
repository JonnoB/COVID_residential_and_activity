{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing polycentric gyration\n",
    "This script tests the code for finding poly centric gyration from the paper [From centre to centres: polycentric structures in individual mobility](https://arxiv.org/abs/2108.08113). Code is from github https://github.com/rohit-sahasrabuddhe/polycentric-mobility . Functions are taken from the main.py file, It would be better to source them in or run the script directly but I don't know how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The packages and functions used by the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from sklearn.cluster import KMeans\n",
    "from haversine import haversine_vector\n",
    "from sklearn.metrics import auc\n",
    "from scipy.spatial import distance_matrix as DM\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Functions for conversion from latlon to cartesian and back\n",
    "def to_cartesian(lat, lon):\n",
    "    lat, lon = np.pi * lat / 180, np.pi * lon / 180\n",
    "    return np.cos(lat) * np.cos(lon), np.cos(lat) * np.sin(lon), np.sin(lat)\n",
    "def to_latlon(x,y,z):\n",
    "    lat, lon = np.arctan2(z, np.sqrt(x**2+y**2))*180/np.pi, np.arctan2(y, x)*180/np.pi\n",
    "    return lat, lon\n",
    "\n",
    "class TrimmedKMeans:\n",
    "    def __init__(self, k, data, weights, cutoff):\n",
    "        self.k = k\n",
    "        self.data = data #A numpy array of size [N, 3]\n",
    "        self.weights = weights / np.sum(weights) #size [N,]\n",
    "        self.centers = self.data[np.random.choice(range(self.data.shape[0]), size=k, replace=False)]\n",
    "        \n",
    "        self.distance_matrix = DM(self.data, self.centers)\n",
    "        self.cluster_assignment = np.argmin(self.distance_matrix, axis=1)\n",
    "        self.distance = np.min(self.distance_matrix, axis=1)\n",
    "        self.inertia = 0\n",
    "        \n",
    "        self.cutoff=cutoff\n",
    "        \n",
    "    def get_inertia_labels(self):\n",
    "        self.distance_matrix = DM(self.data, self.centers)\n",
    "        self.cluster_assignment = np.argmin(self.distance_matrix, axis=1)\n",
    "        self.distance = np.min(self.distance_matrix, axis=1)\n",
    "        self.inertia = 0\n",
    "        for i in range(self.k): # Loop through all the clusters\n",
    "            # get the coordinates, global weights and distance to center\n",
    "            coords, weights, dists = self.data[self.cluster_assignment == i], self.weights[self.cluster_assignment == i], self.distance[self.cluster_assignment == i]\n",
    "            if coords.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            indices_asc = np.argsort(dists)\n",
    "            coords, weights, dists = coords[indices_asc], weights[indices_asc], dists[indices_asc] # sort everything by the distance\n",
    "            cluster_wt = np.sum(weights) # total weight of the cluster\n",
    "            weights = weights / cluster_wt # this gives the local weight (within the cluster)\n",
    "            weights_cumsum = np.cumsum(weights)\n",
    "            \n",
    "            last_entry = np.sum(weights_cumsum <= self.cutoff) + 1 # the index of the last location that needs to be looked at\n",
    "            coords, weights, dists, weights_cumsum = coords[:last_entry].copy(), weights[:last_entry].copy(), dists[:last_entry].copy(), weights_cumsum[:last_entry].copy()\n",
    "            # Remove the extra weight\n",
    "            weights[-1] -= weights_cumsum[-1] - self.cutoff\n",
    "            # Add to the inertia\n",
    "            self.inertia += np.sum((weights * cluster_wt) * (dists**2))\n",
    "        return np.sqrt(self.inertia), self.cluster_assignment\n",
    "        \n",
    "    def update(self):\n",
    "        self.distance_matrix = DM(self.data, self.centers)\n",
    "        self.cluster_assignment = np.argmin(self.distance_matrix, axis=1)\n",
    "        self.distance = np.min(self.distance_matrix, axis=1)\n",
    "        \n",
    "        for i in range(self.k): # Loop through all the clusters\n",
    "            # get the coordinates, global weights and distance to center\n",
    "            coords, weights, dists = self.data[self.cluster_assignment == i], self.weights[self.cluster_assignment == i], self.distance[self.cluster_assignment == i]\n",
    "            if coords.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            indices_asc = np.argsort(dists)\n",
    "            coords, weights, dists = coords[indices_asc], weights[indices_asc], dists[indices_asc] # sort everything by the distance\n",
    "            cluster_wt = np.sum(weights) # total weight of the cluster\n",
    "            weights = weights / cluster_wt # this gives the local weight (within the cluster)\n",
    "            weights_cumsum = np.cumsum(weights)\n",
    "            # last entry is the index of the last location that needs to be looked at\n",
    "            last_entry = np.sum(weights_cumsum <= self.cutoff) + 1\n",
    "            coords, weights, dists, weights_cumsum = coords[:last_entry].copy(), weights[:last_entry].copy(), dists[:last_entry].copy(), weights_cumsum[:last_entry].copy()\n",
    "            # Remove the extra weight\n",
    "            weights[-1] -= weights_cumsum[-1] - self.cutoff\n",
    "            \n",
    "            # Update the center\n",
    "            weights = weights / np.sum(weights)\n",
    "            self.centers[i] = np.average(coords, axis=0, weights=weights)       \n",
    "        \n",
    "\n",
    "    def plot(self):\n",
    "        for i in range(self.k):\n",
    "            plt.scatter(self.data[self.cluster_assignment == i][:, 0], self.data[self.cluster_assignment == i][:, 1])\n",
    "        plt.scatter(self.centers[:, 0], self.centers[:, 1], marker='+', color='black', s=50)\n",
    "    \n",
    "    def get_best_fit(self):\n",
    "        best_centers, best_inertia, best_labels = None , np.inf, None\n",
    "        for _ in range(50): #compare across 50 random initializations\n",
    "            c = np.inf\n",
    "            self.centers = self.data[np.random.choice(range(self.data.shape[0]), size=self.k, replace=False)]\n",
    "            for _ in range(50): #fixed number of iterations\n",
    "                old_c = np.copy(self.centers)\n",
    "                self.update()\n",
    "                c = np.sum((self.centers - old_c)**2)\n",
    "                if c == 0:\n",
    "                    break\n",
    "            this_inertia, this_labels = self.get_inertia_labels()\n",
    "            if this_inertia < best_inertia:\n",
    "                best_inertia = this_inertia\n",
    "                best_labels = this_labels\n",
    "                best_centers = self.centers\n",
    "            if best_inertia == 0:\n",
    "                break\n",
    "            \n",
    "        return best_centers, best_labels, best_inertia\n",
    "    \n",
    "\n",
    "def get_result(u, user_data, locs, max_k, trimming_coeff):\n",
    "    #print(f\"User {u}, {to_print}\")\n",
    "    result = {'user':u, 'com':None, 'tcom':None, 'rog':None, 'L1':None, 'L2':None, 'k':None, 'centers':None, 'auc_com':None, 'auc_1':None, 'auc_2':None, 'auc_k':None, 'auc_kmeans':None}\n",
    "    def get_area_auc(x, k, max_area, df):\n",
    "        centers = x\n",
    "        dists = np.min(haversine_vector(list(df.coords), centers, comb=True), axis=0)\n",
    "        df['distance'] = dists\n",
    "        df['area'] = k * df['distance']**2\n",
    "        df = df.sort_values('area')[['area', 'time_spent']]        \n",
    "        df = df[df['area'] <= max_area]\n",
    "        if df.empty:\n",
    "            return 0        \n",
    "        df.time_spent = df.time_spent.cumsum()        \n",
    "        df['area'] = df['area'] / max_area\n",
    "        x = [0] + list(df['area']) + [1]\n",
    "        y = [0] + list(df.time_spent) + [list(df.time_spent)[-1]]\n",
    "        return auc(x, y)\n",
    "        \n",
    "    user_data = user_data[['loc', 'time_spent']].groupby('loc').sum()\n",
    "    try:\n",
    "        user_data.time_spent = user_data.time_spent.dt.total_seconds()\n",
    "    except:\n",
    "        pass\n",
    "    user_data.time_spent = user_data.time_spent / user_data.time_spent.sum()\n",
    "    user_data['lat'] = locs.loc[user_data.index].lat\n",
    "    user_data['lon'] = locs.loc[user_data.index].lon\n",
    "    \n",
    "    highest_gap = None\n",
    "    best_auc = None\n",
    "    best_gap = None\n",
    "    best_k = 1\n",
    "    best_centers = None\n",
    "    \n",
    "    \n",
    "    user_data['coords'] = list(zip(user_data.lat, user_data.lon))        \n",
    "    user_data['x'], user_data['y'], user_data['z'] = to_cartesian(user_data['lat'], user_data['lon'])\n",
    "    com = to_latlon(np.sum(user_data['x']*user_data.time_spent), np.sum(user_data['y']*user_data.time_spent), np.sum(user_data['z']*user_data.time_spent))\n",
    "    dist = haversine_vector(list(user_data.coords), [com], comb=True)\n",
    "    rog = np.sqrt(np.sum(user_data.time_spent.to_numpy() * (dist**2)))\n",
    "    com_auc = get_area_auc(com, 1, rog**2, user_data.copy())\n",
    "    \n",
    "    \n",
    "    result['com'] = com\n",
    "    result['rog'] = rog\n",
    "    result['L1'], result['L2'] = list(user_data.sort_values('time_spent', ascending=False).coords[:2])\n",
    "    result['auc_com'] = com_auc\n",
    "    \n",
    "    \n",
    "    train_data_list = []\n",
    "    # find max min and shape outside loop\n",
    "    lat_min, lat_max = user_data.lat.min(), user_data.lat.max()\n",
    "    lon_min, lon_max = user_data.lon.min(), user_data.lon.max()\n",
    "    size = user_data.shape[0]\n",
    "    for i in range(50):\n",
    "        train_data = user_data.copy()\n",
    "        train_data['lat'] = np.random.uniform(low=lat_min, high=lat_max, size=size)\n",
    "        train_data['lon'] = np.random.uniform(low=lon_min, high=lon_max, size=size)\n",
    "        train_data['coords'] = list(zip(train_data.lat, train_data.lon))        \n",
    "        train_data['x'], train_data['y'], train_data['z'] = to_cartesian(train_data['lat'], train_data['lon'])\n",
    "            \n",
    "        #find rog of this data\n",
    "        com = to_latlon(np.sum(train_data['x']*train_data.time_spent), np.sum(train_data['y']*train_data.time_spent), np.sum(train_data['z']*train_data.time_spent))\n",
    "        dist = haversine_vector(list(train_data.coords), [com], comb=True)\n",
    "        train_rog = np.sqrt(np.sum(train_data.time_spent.to_numpy() * (dist**2)))   \n",
    "        \n",
    "        train_data_list.append((train_data, train_rog))\n",
    "    \n",
    "    \n",
    "    for k in range(1, max_k+1):   \n",
    "        Trim = TrimmedKMeans(k, user_data[['x','y', 'z']].to_numpy(), weights = user_data.time_spent.to_numpy(), cutoff=trimming_coeff)\n",
    "        true_centers, _, _ = Trim.get_best_fit()        \n",
    "        true_centers = np.array([np.array(to_latlon(*i)) for i in true_centers])\n",
    "        true_auc = get_area_auc(true_centers, k, rog**2, user_data.copy())\n",
    "        \n",
    "        if k == 1:\n",
    "            result['tcom'] = tuple(true_centers[0])\n",
    "            result['auc_1'] = true_auc\n",
    "        if k== 2:\n",
    "            result['auc_2'] = true_auc\n",
    "        \n",
    "        new_aucs = []\n",
    "        for train_data, train_rog in train_data_list:\n",
    "            Trim = TrimmedKMeans(k, train_data[['x','y', 'z']].to_numpy(), weights = train_data.time_spent.to_numpy(), cutoff=trimming_coeff)\n",
    "            centers, _, _ = Trim.get_best_fit()        \n",
    "            centers = np.array([np.array(to_latlon(*i)) for i in centers])\n",
    "            new_aucs.append(get_area_auc(centers, k, train_rog**2, train_data.copy()))\n",
    "            \n",
    "            \n",
    "        new_mean = np.mean(new_aucs)\n",
    "        new_std = np.std(new_aucs)        \n",
    "        gap = true_auc - new_mean\n",
    "        \n",
    "        if k == 1:\n",
    "            highest_gap = gap\n",
    "            best_gap = gap\n",
    "            best_auc = true_auc\n",
    "            best_centers = true_centers\n",
    "            best_k = 1\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if gap - new_std > highest_gap:\n",
    "            best_auc = true_auc\n",
    "            best_gap = gap\n",
    "            best_centers = true_centers\n",
    "            best_k = k\n",
    "        highest_gap = max(highest_gap, gap)\n",
    "    \n",
    "    result['k'] = best_k\n",
    "    result['auc_k'], result['centers'] = best_auc, list(best_centers)\n",
    "    \n",
    "    kmeans = KMeans(result['k'])\n",
    "    kmeans.fit(user_data[['x','y', 'z']].to_numpy(), sample_weight = user_data.time_spent.to_numpy())\n",
    "    kmeans_centers = np.array([np.array(to_latlon(*i)) for i in kmeans.cluster_centers_])\n",
    "    result['auc_kmeans'] = get_area_auc(kmeans_centers, result['k'], rog**2, user_data.copy())\n",
    "    return result\n",
    "\n",
    "def main(data_path, results_path=\"demo_results.pkl\", max_k=6, trimming_coeff=0.9):\n",
    "    data = pd.read_pickle(data_path)\n",
    "    try:\n",
    "        data['time_spent'] = data['end_time'] - data['start_time']\n",
    "    except:\n",
    "        pass\n",
    "    user_list = sorted(data.user.unique())\n",
    "    locs = data[['loc', 'lat', 'lon']].groupby('loc').mean().copy()\n",
    "    \n",
    "    result = pd.DataFrame(Parallel(n_jobs=-1)(delayed(get_result)(u, data[data.user == u], locs, max_k, trimming_coeff) for u in user_list)).set_index('user')\n",
    "    result.to_pickle(results_path)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load mobility csv and save as pandas dataframe\n",
    "\n",
    "These code chunks create the appropriately formatted dataframe from a csv of the mobility data produced in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          loc        lat       lon  time_spent  user\n",
      "0         842  51.521712 -0.047056           1   172\n",
      "1         842  51.521971 -0.046572           1   390\n",
      "2         847  51.521690 -0.047084           1   263\n",
      "3          74  51.469348  0.119686           1    69\n",
      "4         841  51.521621 -0.043450           1   710\n",
      "...       ...        ...       ...         ...   ...\n",
      "24936986  242  51.528828 -0.349071           1   242\n",
      "24936987  376  51.478107 -0.194349           1   376\n",
      "24936988  128  51.412014 -0.034207           1   133\n",
      "24936989   46  51.601814 -0.235075           1    34\n",
      "24936990  295  51.620292 -0.070886           1   282\n",
      "\n",
      "[24936991 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "dis_pandas = pd.read_csv(\"/home/jonno/COVID_project/COVID_project_data/poly_df.csv\").loc[:,['loc','lat','lon', 'time_spent', 'user']]\n",
    "dis_pandas['time_spent'] = dis_pandas['time_spent'].astype('float')\n",
    "dis_pandas.to_pickle(\"/home/jonno/COVID_project/COVID_project_data/poly_df.pkl\")\n",
    "del dis_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create smaller file that will be easier to test\n",
    "user_list = [i for i in range(10)]\n",
    "dis_pandas[dis_pandas[\"user\"].isin(user_list)].to_pickle(\"/home/jonno/COVID_project/COVID_project_data/poly_df2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = \"/home/jonno/polycentric-mobility/main.py\"\n",
    "target_file_path = \"/home/jonno/COVID_project/COVID_project_data/poly_df.pkl\"#\n",
    "demo_file_path = \"/home/jonno/polycentric-mobility/demo_data.pkl\"\n",
    "result_save_path = \"/home/jonno/COVID_project/COVID_project_data/multi_gyration.pkl\"\n",
    "\n",
    "#!python /home/jonno/polycentric-mobility/main.py --data_path \"{target_file_path}\" --results_path \"{result_save_path}\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test and demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.read_pickle(\"/home/jonno/COVID_project/COVID_project_data/poly_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      loc        lat        lon  time_spent  user                   geometry\n",
      "0      53  55.830584  12.392003           1     1  POINT (12.39200 55.83058)\n",
      "1       9  55.827128  12.389890           1     1  POINT (12.38989 55.82713)\n",
      "2      23  55.728763  12.504619           1     1  POINT (12.50462 55.72876)\n",
      "3      84  55.759575  12.362043           1     1  POINT (12.36204 55.75958)\n",
      "4      88  55.773512  12.447521           1     1  POINT (12.44752 55.77351)\n",
      "...   ...        ...        ...         ...   ...                        ...\n",
      "1795  270  55.372064  12.000921           1     3  POINT (12.00092 55.37206)\n",
      "1796  107  55.508729  11.492881           1     3  POINT (11.49288 55.50873)\n",
      "1797  298  55.340643  11.977785           1     3  POINT (11.97779 55.34064)\n",
      "1798   46  55.776968  12.327596           1     3  POINT (12.32760 55.77697)\n",
      "1799   98  55.800256  12.394273           1     3  POINT (12.39427 55.80026)\n",
      "\n",
      "[1800 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "demo_df = pd.read_pickle(\"/home/jonno/polycentric-mobility/demo_data.pkl\")\n",
    "print(demo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing data types\n",
    "The data types and the column names for the arguements are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loc             int64\n",
       "lat           float64\n",
       "lon           float64\n",
       "time_spent      int64\n",
       "user            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loc              int64\n",
       "lat            float64\n",
       "lon            float64\n",
       "time_spent       int64\n",
       "user             int64\n",
       "geometry      geometry\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 103.04703950881958 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Demo data succeeds\n",
    "import time\n",
    "start_time = time.time()\n",
    "main(data_path = demo_file_path, results_path = result_save_path)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 23237.76428437233 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#The real data fails\n",
    "import time\n",
    "start_time = time.time()\n",
    "main(data_path = target_file_path, results_path = result_save_path,  max_k=6)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(result_save_path).to_csv(\"/home/jonno/COVID_project/COVID_project_data/multi_gyration.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            com  \\\n",
      "user                                              \n",
      "0     (51.50936611114374, -0.07897089589632829)   \n",
      "1      (51.57028021391862, 0.10676118654879706)   \n",
      "2       (51.5616122600484, 0.09884438706657804)   \n",
      "3      (51.560455660524546, 0.1477798251896961)   \n",
      "4      (51.55068770444214, 0.09347319403867203)   \n",
      "5     (51.551550170741265, 0.11085328909507795)   \n",
      "6      (51.54382176759679, 0.08350875388764721)   \n",
      "7       (51.54708765584114, 0.0867602580508517)   \n",
      "8      (51.54600246715115, 0.10311999488752895)   \n",
      "9     (51.544359444615736, 0.10407477063155353)   \n",
      "\n",
      "                                           tcom       rog  \\\n",
      "user                                                        \n",
      "0     (51.51464830725228, -0.08204290624543803)  7.813708   \n",
      "1      (51.57494813308364, 0.12754345935861824)  6.449312   \n",
      "2      (51.56575968766957, 0.12343755320683983)  6.865981   \n",
      "3      (51.56416987651819, 0.17221942235717372)  6.359004   \n",
      "4      (51.55510174818023, 0.11836729221912884)  7.449399   \n",
      "5      (51.55490740697243, 0.13837246017079782)  7.362199   \n",
      "6      (51.551729991132795, 0.1134758617587256)  9.099581   \n",
      "7      (51.55060978070238, 0.11094336677287403)  6.665786   \n",
      "8      (51.54810816005073, 0.12953517619556426)  7.145297   \n",
      "9       (51.54561837601179, 0.1330768794556477)  7.929840   \n",
      "\n",
      "                                             L1  \\\n",
      "user                                              \n",
      "0     (51.51507480779646, -0.09105568287441552)   \n",
      "1      (51.58349908620117, 0.13649230284638827)   \n",
      "2       (51.572030480614444, 0.139615628949705)   \n",
      "3      (51.56004158351096, 0.17499549778150847)   \n",
      "4      (51.56065093483202, 0.14425312500681287)   \n",
      "5       (51.5572233866825, 0.15467280327880542)   \n",
      "6       (51.55275011402613, 0.1358869611896268)   \n",
      "7      (51.551240203654686, 0.1183266093559444)   \n",
      "8      (51.54742242719527, 0.14881339956936623)   \n",
      "9     (51.546160540012295, 0.16467680268807916)   \n",
      "\n",
      "                                             L2  k  \\\n",
      "user                                                 \n",
      "0     (51.51008733358576, -0.12663148843253766)  1   \n",
      "1       (51.572030480614444, 0.139615628949705)  1   \n",
      "2      (51.58349908620117, 0.13649230284638827)  1   \n",
      "3      (51.56800087054753, 0.17234170171836397)  1   \n",
      "4       (51.55275011402613, 0.1358869611896268)  1   \n",
      "5      (51.56065093483202, 0.14425312500681287)  1   \n",
      "6      (51.56065093483202, 0.14425312500681287)  1   \n",
      "7       (51.55275011402613, 0.1358869611896268)  1   \n",
      "8      (51.54184028315769, 0.14695852780424476)  1   \n",
      "9      (51.53940432600126, 0.15940554437149107)  1   \n",
      "\n",
      "                                          centers   auc_com     auc_1  \\\n",
      "user                                                                    \n",
      "0     [[51.51464830725228, -0.08204290624543803]]  0.643937  0.646890   \n",
      "1      [[51.57494813308364, 0.12754345935861824]]  0.604385  0.658222   \n",
      "2      [[51.56575968766957, 0.12343755320683983]]  0.612173  0.694825   \n",
      "3      [[51.56416987651819, 0.17221942235717372]]  0.668636  0.739749   \n",
      "4      [[51.55510174818023, 0.11836729221912884]]  0.556257  0.656742   \n",
      "5      [[51.55490740697243, 0.13837246017079782]]  0.625897  0.718059   \n",
      "6      [[51.551729991132795, 0.1134758617587256]]  0.603465  0.693620   \n",
      "7      [[51.55060978070238, 0.11094336677287403]]  0.618167  0.692238   \n",
      "8      [[51.54810816005073, 0.12953517619556426]]  0.586310  0.687928   \n",
      "9       [[51.54561837601179, 0.1330768794556477]]  0.571392  0.671274   \n",
      "\n",
      "         auc_2     auc_k  auc_kmeans  \n",
      "user                                  \n",
      "0     0.629364  0.646890    0.643937  \n",
      "1     0.612142  0.658222    0.604385  \n",
      "2     0.669515  0.694825    0.612173  \n",
      "3     0.682375  0.739749    0.668636  \n",
      "4     0.671424  0.656742    0.556257  \n",
      "5     0.703536  0.718059    0.625897  \n",
      "6     0.664813  0.693620    0.603465  \n",
      "7     0.650555  0.692238    0.618167  \n",
      "8     0.672623  0.687928    0.586310  \n",
      "9     0.677272  0.671274    0.571392  \n"
     ]
    }
   ],
   "source": [
    "multi_gyration = pd.read_pickle('/home/jonno/COVID_project/COVID_project_data/multi_gyration_test.pkl')\n",
    "print(multi_gyration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
