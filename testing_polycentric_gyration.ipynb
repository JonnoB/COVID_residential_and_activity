{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing polycentric gyration\n",
    "This script tests the code for finding poly centric gyration from the paper [From centre to centres: polycentric structures in individual mobility](https://arxiv.org/abs/2108.08113). Code is from github https://github.com/rohit-sahasrabuddhe . Functions are taken from the main.py file, It would be better to source them in or run the script directly but I don't know how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The packages and functions used by the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from sklearn.cluster import KMeans\n",
    "from haversine import haversine_vector\n",
    "from sklearn.metrics import auc\n",
    "from scipy.spatial import distance_matrix as DM\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Functions for conversion from latlon to cartesian and back\n",
    "def to_cartesian(lat, lon):\n",
    "    lat, lon = np.pi * lat / 180, np.pi * lon / 180\n",
    "    return np.cos(lat) * np.cos(lon), np.cos(lat) * np.sin(lon), np.sin(lat)\n",
    "def to_latlon(x,y,z):\n",
    "    lat, lon = np.arctan2(z, np.sqrt(x**2+y**2))*180/np.pi, np.arctan2(y, x)*180/np.pi\n",
    "    return lat, lon\n",
    "\n",
    "class TrimmedKMeans:\n",
    "    def __init__(self, k, data, weights, cutoff):\n",
    "        self.k = k\n",
    "        self.data = data #A numpy array of size [N, 3]\n",
    "        self.weights = weights / np.sum(weights) #size [N,]\n",
    "        self.centers = self.data[np.random.choice(range(self.data.shape[0]), size=k, replace=False)]\n",
    "        \n",
    "        self.distance_matrix = DM(self.data, self.centers)\n",
    "        self.cluster_assignment = np.argmin(self.distance_matrix, axis=1)\n",
    "        self.distance = np.min(self.distance_matrix, axis=1)\n",
    "        self.inertia = 0\n",
    "        \n",
    "        self.cutoff=cutoff\n",
    "        \n",
    "    def get_inertia_labels(self):\n",
    "        self.distance_matrix = DM(self.data, self.centers)\n",
    "        self.cluster_assignment = np.argmin(self.distance_matrix, axis=1)\n",
    "        self.distance = np.min(self.distance_matrix, axis=1)\n",
    "        self.inertia = 0\n",
    "        for i in range(self.k): # Loop through all the clusters\n",
    "            # get the coordinates, global weights and distance to center\n",
    "            coords, weights, dists = self.data[self.cluster_assignment == i], self.weights[self.cluster_assignment == i], self.distance[self.cluster_assignment == i]\n",
    "            if coords.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            indices_asc = np.argsort(dists)\n",
    "            coords, weights, dists = coords[indices_asc], weights[indices_asc], dists[indices_asc] # sort everything by the distance\n",
    "            cluster_wt = np.sum(weights) # total weight of the cluster\n",
    "            weights = weights / cluster_wt # this gives the local weight (within the cluster)\n",
    "            weights_cumsum = np.cumsum(weights)\n",
    "            \n",
    "            last_entry = np.sum(weights_cumsum <= self.cutoff) + 1 # the index of the last location that needs to be looked at\n",
    "            coords, weights, dists, weights_cumsum = coords[:last_entry].copy(), weights[:last_entry].copy(), dists[:last_entry].copy(), weights_cumsum[:last_entry].copy()\n",
    "            # Remove the extra weight\n",
    "            weights[-1] -= weights_cumsum[-1] - self.cutoff\n",
    "            # Add to the inertia\n",
    "            self.inertia += np.sum((weights * cluster_wt) * (dists**2))\n",
    "        return np.sqrt(self.inertia), self.cluster_assignment\n",
    "        \n",
    "    def update(self):\n",
    "        self.distance_matrix = DM(self.data, self.centers)\n",
    "        self.cluster_assignment = np.argmin(self.distance_matrix, axis=1)\n",
    "        self.distance = np.min(self.distance_matrix, axis=1)\n",
    "        \n",
    "        for i in range(self.k): # Loop through all the clusters\n",
    "            # get the coordinates, global weights and distance to center\n",
    "            coords, weights, dists = self.data[self.cluster_assignment == i], self.weights[self.cluster_assignment == i], self.distance[self.cluster_assignment == i]\n",
    "            if coords.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            indices_asc = np.argsort(dists)\n",
    "            coords, weights, dists = coords[indices_asc], weights[indices_asc], dists[indices_asc] # sort everything by the distance\n",
    "            cluster_wt = np.sum(weights) # total weight of the cluster\n",
    "            weights = weights / cluster_wt # this gives the local weight (within the cluster)\n",
    "            weights_cumsum = np.cumsum(weights)\n",
    "            # last entry is the index of the last location that needs to be looked at\n",
    "            last_entry = np.sum(weights_cumsum <= self.cutoff) + 1\n",
    "            coords, weights, dists, weights_cumsum = coords[:last_entry].copy(), weights[:last_entry].copy(), dists[:last_entry].copy(), weights_cumsum[:last_entry].copy()\n",
    "            # Remove the extra weight\n",
    "            weights[-1] -= weights_cumsum[-1] - self.cutoff\n",
    "            \n",
    "            # Update the center\n",
    "            weights = weights / np.sum(weights)\n",
    "            self.centers[i] = np.average(coords, axis=0, weights=weights)       \n",
    "        \n",
    "\n",
    "    def plot(self):\n",
    "        for i in range(self.k):\n",
    "            plt.scatter(self.data[self.cluster_assignment == i][:, 0], self.data[self.cluster_assignment == i][:, 1])\n",
    "        plt.scatter(self.centers[:, 0], self.centers[:, 1], marker='+', color='black', s=50)\n",
    "    \n",
    "    def get_best_fit(self):\n",
    "        best_centers, best_inertia, best_labels = None , np.inf, None\n",
    "        for _ in range(50): #compare across 50 random initializations\n",
    "            c = np.inf\n",
    "            self.centers = self.data[np.random.choice(range(self.data.shape[0]), size=self.k, replace=False)]\n",
    "            for _ in range(50): #fixed number of iterations\n",
    "                old_c = np.copy(self.centers)\n",
    "                self.update()\n",
    "                c = np.sum((self.centers - old_c)**2)\n",
    "                if c == 0:\n",
    "                    break\n",
    "            this_inertia, this_labels = self.get_inertia_labels()\n",
    "            if this_inertia < best_inertia:\n",
    "                best_inertia = this_inertia\n",
    "                best_labels = this_labels\n",
    "                best_centers = self.centers\n",
    "            if best_inertia == 0:\n",
    "                break\n",
    "            \n",
    "        return best_centers, best_labels, best_inertia\n",
    "    \n",
    "\n",
    "def get_result(u, user_data, locs, max_k, trimming_coeff):\n",
    "    #print(f\"User {u}, {to_print}\")\n",
    "    result = {'user':u, 'com':None, 'tcom':None, 'rog':None, 'L1':None, 'L2':None, 'k':None, 'centers':None, 'auc_com':None, 'auc_1':None, 'auc_2':None, 'auc_k':None, 'auc_kmeans':None}\n",
    "    def get_area_auc(x, k, max_area, df):\n",
    "        centers = x\n",
    "        dists = np.min(haversine_vector(list(df.coords), centers, comb=True), axis=0)\n",
    "        df['distance'] = dists\n",
    "        df['area'] = k * df['distance']**2\n",
    "        df = df.sort_values('area')[['area', 'time_spent']]        \n",
    "        df = df[df['area'] <= max_area]\n",
    "        if df.empty:\n",
    "            return 0        \n",
    "        df.time_spent = df.time_spent.cumsum()        \n",
    "        df['area'] = df['area'] / max_area\n",
    "        x = [0] + list(df['area']) + [1]\n",
    "        y = [0] + list(df.time_spent) + [list(df.time_spent)[-1]]\n",
    "        return auc(x, y)\n",
    "        \n",
    "    user_data = user_data[['loc', 'time_spent']].groupby('loc').sum()\n",
    "    try:\n",
    "        user_data.time_spent = user_data.time_spent.dt.total_seconds()\n",
    "    except:\n",
    "        pass\n",
    "    user_data.time_spent = user_data.time_spent / user_data.time_spent.sum()\n",
    "    user_data['lat'] = locs.loc[user_data.index].lat\n",
    "    user_data['lon'] = locs.loc[user_data.index].lon\n",
    "    \n",
    "    highest_gap = None\n",
    "    best_auc = None\n",
    "    best_gap = None\n",
    "    best_k = 1\n",
    "    best_centers = None\n",
    "    \n",
    "    \n",
    "    user_data['coords'] = list(zip(user_data.lat, user_data.lon))        \n",
    "    user_data['x'], user_data['y'], user_data['z'] = to_cartesian(user_data['lat'], user_data['lon'])\n",
    "    com = to_latlon(np.sum(user_data['x']*user_data.time_spent), np.sum(user_data['y']*user_data.time_spent), np.sum(user_data['z']*user_data.time_spent))\n",
    "    dist = haversine_vector(list(user_data.coords), [com], comb=True)\n",
    "    rog = np.sqrt(np.sum(user_data.time_spent.to_numpy() * (dist**2)))\n",
    "    com_auc = get_area_auc(com, 1, rog**2, user_data.copy())\n",
    "    \n",
    "    \n",
    "    result['com'] = com\n",
    "    result['rog'] = rog\n",
    "    result['L1'], result['L2'] = list(user_data.sort_values('time_spent', ascending=False).coords[:2])\n",
    "    result['auc_com'] = com_auc\n",
    "    \n",
    "    \n",
    "    train_data_list = []\n",
    "    # find max min and shape outside loop\n",
    "    lat_min, lat_max = user_data.lat.min(), user_data.lat.max()\n",
    "    lon_min, lon_max = user_data.lon.min(), user_data.lon.max()\n",
    "    size = user_data.shape[0]\n",
    "    for i in range(50):\n",
    "        train_data = user_data.copy()\n",
    "        train_data['lat'] = np.random.uniform(low=lat_min, high=lat_max, size=size)\n",
    "        train_data['lon'] = np.random.uniform(low=lon_min, high=lon_max, size=size)\n",
    "        train_data['coords'] = list(zip(train_data.lat, train_data.lon))        \n",
    "        train_data['x'], train_data['y'], train_data['z'] = to_cartesian(train_data['lat'], train_data['lon'])\n",
    "            \n",
    "        #find rog of this data\n",
    "        com = to_latlon(np.sum(train_data['x']*train_data.time_spent), np.sum(train_data['y']*train_data.time_spent), np.sum(train_data['z']*train_data.time_spent))\n",
    "        dist = haversine_vector(list(train_data.coords), [com], comb=True)\n",
    "        train_rog = np.sqrt(np.sum(train_data.time_spent.to_numpy() * (dist**2)))   \n",
    "        \n",
    "        train_data_list.append((train_data, train_rog))\n",
    "    \n",
    "    \n",
    "    for k in range(1, max_k+1):   \n",
    "        Trim = TrimmedKMeans(k, user_data[['x','y', 'z']].to_numpy(), weights = user_data.time_spent.to_numpy(), cutoff=trimming_coeff)\n",
    "        true_centers, _, _ = Trim.get_best_fit()        \n",
    "        true_centers = np.array([np.array(to_latlon(*i)) for i in true_centers])\n",
    "        true_auc = get_area_auc(true_centers, k, rog**2, user_data.copy())\n",
    "        \n",
    "        if k == 1:\n",
    "            result['tcom'] = tuple(true_centers[0])\n",
    "            result['auc_1'] = true_auc\n",
    "        if k== 2:\n",
    "            result['auc_2'] = true_auc\n",
    "        \n",
    "        new_aucs = []\n",
    "        for train_data, train_rog in train_data_list:\n",
    "            Trim = TrimmedKMeans(k, train_data[['x','y', 'z']].to_numpy(), weights = train_data.time_spent.to_numpy(), cutoff=trimming_coeff)\n",
    "            centers, _, _ = Trim.get_best_fit()        \n",
    "            centers = np.array([np.array(to_latlon(*i)) for i in centers])\n",
    "            new_aucs.append(get_area_auc(centers, k, train_rog**2, train_data.copy()))\n",
    "            \n",
    "            \n",
    "        new_mean = np.mean(new_aucs)\n",
    "        new_std = np.std(new_aucs)        \n",
    "        gap = true_auc - new_mean\n",
    "        \n",
    "        if k == 1:\n",
    "            highest_gap = gap\n",
    "            best_gap = gap\n",
    "            best_auc = true_auc\n",
    "            best_centers = true_centers\n",
    "            best_k = 1\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if gap - new_std > highest_gap:\n",
    "            best_auc = true_auc\n",
    "            best_gap = gap\n",
    "            best_centers = true_centers\n",
    "            best_k = k\n",
    "        highest_gap = max(highest_gap, gap)\n",
    "    \n",
    "    result['k'] = best_k\n",
    "    result['auc_k'], result['centers'] = best_auc, list(best_centers)\n",
    "    \n",
    "    kmeans = KMeans(result['k'])\n",
    "    kmeans.fit(user_data[['x','y', 'z']].to_numpy(), sample_weight = user_data.time_spent.to_numpy())\n",
    "    kmeans_centers = np.array([np.array(to_latlon(*i)) for i in kmeans.cluster_centers_])\n",
    "    result['auc_kmeans'] = get_area_auc(kmeans_centers, result['k'], rog**2, user_data.copy())\n",
    "    return result\n",
    "\n",
    "def main(data_path, results_path=\"demo_results.pkl\", max_k=6, trimming_coeff=0.9):\n",
    "    data = pd.read_pickle(data_path)\n",
    "    #print(\"file loaded\") #added line\n",
    "    try:\n",
    "        data['time_spent'] = data['end_time'] - data['start_time']\n",
    "    except:\n",
    "        pass\n",
    "    user_list = sorted(data.user.unique())\n",
    "    locs = data[['loc', 'lat', 'lon']].groupby('loc').mean().copy()\n",
    "    #print(locs) #added line\n",
    "    result = pd.DataFrame(Parallel(n_jobs=-1)(delayed(get_result)(u, data[data.user == u], locs, max_k, trimming_coeff) for u in user_list)).set_index('user')\n",
    "    result.to_pickle(results_path)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load mobility csv and save as pandas dataframe\n",
    "\n",
    "These code chunks create the appropriately formatted dataframe from a csv of the mobility data produced in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_pandas = pd.read_csv(\"/home/jonno/COVID_project/COVID_project_data/poly_df.csv\").loc[:,['loc','lat','lon', 'time_spent', 'user']]\n",
    "#dis_pandas['time_spent'] = dis_pandas['time_spent'].astype('float')\n",
    "dis_pandas.to_pickle(\"/home/jonno/COVID_project/COVID_project_data/poly_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create smaller file that will be easier to test\n",
    "user_list = [i for i in range(10)]\n",
    "dis_pandas[dis_pandas[\"user\"].isin(user_list)].to_pickle(\"/home/jonno/COVID_project/COVID_project_data/poly_df2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = \"/home/jonno/polycentric-mobility/main.py\"\n",
    "target_file_path = \"/home/jonno/COVID_project/COVID_project_data/poly_df2.pkl\"#\n",
    "demo_file_path = \"/home/jonno/polycentric-mobility/demo_data.pkl\"\n",
    "result_save_path = \"/home/jonno/COVID_project/COVID_project_data/multi_gyration.pkl\"\n",
    "\n",
    "#!python /home/jonno/polycentric-mobility/main.py --data_path \"{target_file_path}\" --results_path \"{result_save_path}\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test and demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.read_pickle(\"/home/jonno/COVID_project/COVID_project_data/poly_df2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      loc        lat        lon  time_spent  user                   geometry\n",
      "0      53  55.830584  12.392003           1     1  POINT (12.39200 55.83058)\n",
      "1       9  55.827128  12.389890           1     1  POINT (12.38989 55.82713)\n",
      "2      23  55.728763  12.504619           1     1  POINT (12.50462 55.72876)\n",
      "3      84  55.759575  12.362043           1     1  POINT (12.36204 55.75958)\n",
      "4      88  55.773512  12.447521           1     1  POINT (12.44752 55.77351)\n",
      "...   ...        ...        ...         ...   ...                        ...\n",
      "1795  270  55.372064  12.000921           1     3  POINT (12.00092 55.37206)\n",
      "1796  107  55.508729  11.492881           1     3  POINT (11.49288 55.50873)\n",
      "1797  298  55.340643  11.977785           1     3  POINT (11.97779 55.34064)\n",
      "1798   46  55.776968  12.327596           1     3  POINT (12.32760 55.77697)\n",
      "1799   98  55.800256  12.394273           1     3  POINT (12.39427 55.80026)\n",
      "\n",
      "[1800 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "demo_df = pd.read_pickle(\"/home/jonno/polycentric-mobility/demo_data.pkl\")\n",
    "print(data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing data types\n",
    "The data types and the column names for the arguements are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loc             int64\n",
       "lat           float64\n",
       "lon           float64\n",
       "time_spent      int64\n",
       "user            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loc              int64\n",
       "lat            float64\n",
       "lon            float64\n",
       "time_spent       int64\n",
       "user             int64\n",
       "geometry      geometry\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 127.73041772842407 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Demo data succeeds\n",
    "import time\n",
    "start_time = time.time()\n",
    "main(data_path = demo_file_path, results_path = result_save_path)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/jonno/.local/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/home/jonno/.local/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/jonno/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/jonno/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/jonno/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"<ipython-input-23-b18bd2bc5495>\", line 156, in get_result\nValueError: not enough values to unpack (expected 2, got 1)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d981f677942d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-b18bd2bc5495>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_path, results_path, max_k, trimming_coeff)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;31m#print(locs) #added line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrimming_coeff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#The real data fails\n",
    "import time\n",
    "start_time = time.time()\n",
    "main(data_path = target_file_path, results_path = result_save_path)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
